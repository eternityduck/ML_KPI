{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eternityduck/ML_KPI/blob/main/Lab3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5S4XCUZ6GH7"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BOjujz601HcS"
      },
      "outputs": [],
      "source": [
        "!pip install roman\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "#from google.colab import drive\n",
        "import requests\n",
        "from roman import toRoman\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "\n",
        "\n",
        "text = requests.get('https://www.gutenberg.org/files/11/11-0.txt').text\n",
        "text = text[1517:]\n",
        "print(text[:500])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjC8TSkp6IiH"
      },
      "source": [
        "## Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zswl7jRtGzkk"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "regex = re.compile('[^a-zA-Z \\n]')\n",
        "\n",
        "divided_text = []\n",
        "for i in range(2, 13):\n",
        "  app, text = text.split(f'CHAPTER {toRoman(i)}.')\n",
        "  divided_text.append(regex.sub('', app))\n",
        "divided_text.append(regex.sub('', text.split('THE END')[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1CL6Bh86TS1"
      },
      "source": [
        "## TF-IDF by scikit\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5MFtI3X1FErU"
      },
      "outputs": [],
      "source": [
        "vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
        "vectors = vectorizer.fit_transform(divided_text)\n",
        "features = vectorizer.get_feature_names_out()\n",
        "dense = vectors.todense()\n",
        "denselist = dense.tolist()\n",
        "\n",
        "pd.set_option('display.max_columns', 21)\n",
        "pd.set_option('display.width', 220)\n",
        "\n",
        "df = pd.DataFrame(denselist, columns=features, dtype=\"float32\")\n",
        "for i in range(12):\n",
        "  print(df.sort_values(by=i, axis=1, ascending=False), '\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2YvxWcI6lnB"
      },
      "source": [
        "## LDA from scikit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Db9kkYnOFKYr"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "\n",
        "\n",
        "topwords = []\n",
        "for i in range(12):\n",
        "  count_vect = CountVectorizer(stop_words='english')\n",
        "  doc_term_matrix = count_vect.fit_transform([divided_text[i]])\n",
        "  LDA = LatentDirichletAllocation(n_components=1, random_state=1)\n",
        "  LDA.fit(doc_term_matrix)\n",
        "\n",
        "  ntopwords = 20\n",
        "  tf_feature_names = count_vect.get_feature_names_out()\n",
        "\n",
        "  for topic in LDA.components_:\n",
        "    topwords.append([tf_feature_names[i] for i in topic.argsort()[:-ntopwords - 1:-1]])\n",
        "\n",
        "for words in topwords:\n",
        "  print(words)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Conclusion\n",
        "In our experiments, we experimented with searching top words in book's chapters by removing unnecessary parts of text to analyse and applying two algorithms, one is statistical and second one is neural.\n",
        "\n",
        "TF_IDF is statistical and thus is very straight forward and precise giving just the coefficient of the word usage.\n",
        "\n",
        "LDA is neural algorithm and thus can be imprecise but all depends on training and has more flexibility to be trained on detecting topic words."
      ],
      "metadata": {
        "id": "duIu_KHU2PgE"
      }
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}